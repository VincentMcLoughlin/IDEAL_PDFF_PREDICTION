createinfo.dbri.local

python3 utils/dicom_to_numpy.py -f reference_data/mini_IDEAL_with_extracted_paths.csv -t IDEAL -d /home/mclougv/IDEAL_PDFF_prediction/X20254_mini_dark_files -l /home/mclougv/IDEAL_PDFF_prediction/X20254_mini_light_files

4491437_20254_2_0 -> Bad file 

reference_data/IDEAL_x20254_2_with_Path_No_PDFF_Na.csv -> only files with a PDFF value

IDEAL_x20254_2_clean_with_path -> No PDFF Values of NA, no files with anything other than 36 light and 36 dark samples. Removed 10 files that had wrong number of files, ~10K with NA pdff Values

Remaining files: 33242

Each IDEAL numpy is 8.2 MB for dark and light respectively

8.2 MB * 33242 = 272.6 GB for dark files, 272.6 GB for light files, 545.2 GB total

UNZIPPING
-> It takes a 64s to unzip 100 images

64/100 = 0.64 images per second, 0.64 images/second * 33242 images / 60 seconds / minute = 355 minutes = 5 hours 55 minutes per full unzipping PLUS processing time per batch of images. Cannot be done on a per image basis.

processing

18/20 = 0.9 images per second, 0.9 images per second * 33242 / 60 = 498.63 minutes = 8 hours, 18 minutes

*************************************************************************
Got the space, srun 

srun -c 8 --mem=70G --gres=gpu:rtx2080:2 python3 main.py

sbatch -c 8 --mem=120G --gres=gpu:3 --wrap="python3 main.py"
scontrol show jobid -dd 2007070

sbatch -c 8 --mem=120G --gres=gpu:4 --wrap="python3 main.py"

Location of max pixel value for dark 
29600 327.0
29500 375.0

slurm-2007264.out - Pretrained resnet50 w imagenet 

slurm-2007297 - untrained resnet20, some success with batch size of 8 but quite unstable